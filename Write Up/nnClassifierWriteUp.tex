\documentclass{article}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{float}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DISCLAIMER COVER SHEET
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WRITE UP FUN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{CS170 Project 2 Write Up: Feature Selection with Nearest Neighbor}
\author{Audrey Der, 861221280}
\date{\today}
\maketitle

\section{Introduction}
This is write up details the second project in Dr. Eamonn Keogh's course on Artifical
Intelligence (Fall 2017). The project explores the success of the nearest neighbor
algorithm under different feature selection searches. Nearest neighbors
is sensitive to irrelevant features, making fruitful to search for
the best set of features when applying the algorithm.

\subsection{Project-Specific Details}
The project calls for the following search algorithms to be implemented: forward selection, backward
elimination, and a custom algorithm from the student. The custom algorithm is
designed to be faster, more accurate, or both, than forward selection and
backward elimination. \\

For the purposes of the assignment, each student was assigned a unique pair of data
sets to consider. One small, with 10 features, and the other large, with 50
features. The files I was to consider were \textbf{CS170Smalltestdata\_\_32.txt}
and \textbf{CS170BIGtestdata\_\_71.txt}. \\

This report includes a full copy of the implementation and a sample output using
my custom algorithm on data set with 50 features and two possible classifications.

\section{Algorithms}
\subsection{Forward Selection}
On a high level, forward selection considers all subsets of features in the
data's given features by adding features to an empty set. It returns the set of
most accurate features.

\subsection{Backward Elimination}
Backward elimination also considers all subsets of features, but begins with the
entire set of given features, and removes features from the set. It returns the
set of most accurate features.

\subsection{Forward's Propinqua: Faster Forward Selection}
\textbf{Propinqua}: (\textit{Latin}) feminine singular form of 'propinquo',
meaning ``next, near'' 

Forward selection returns the best set of features by keeping track of the
current best set it as it considers each subset of features. However, in
principle, nearest neighbors and a set of only the best features $S$ will perform
better than nearest neighbors on $S$ with any \textit{additional} features. \\ 

Consider a set of current best features $S_1$ with an accuracy of $a_1$. Suppose
accuracy begins to decline after the addition of a feature $f_i$ to $S_1$ (call
this set of features $S_{f_i}$). Any feature set $S$ where $S_{f_i}$ is a
subset of $S$ will never yield an accuracy greater than $a_1$. It can be concluded
that exploring any set $S$ containing $S_{f_i}$ is likely to be unfruitful.
This, in theory should significantly decrease the time the algorithm spends
searching for the most accurate set of features. \\

\subsection{Caveats and Deductions: Local Maxima and Why Greedy is Conditionally Acceptable}

My custom algorithm implements that principle in three lines of python. However,
Forward's Propinqua is quite greedy. It is extremely sensitive to local maxima; it breaks should it find
one. Because of the risk of returning a local maxima, some algorithms may opt to
continue searching $n$ many further feature additions in the case a local maxima
was hit (and the global maxima is yet to come), but Forward's Propinqua does not. \\

To reiterate: Forward's Propinqua will miss the true best set after returning a
local maxima. Despite this incredible blind spot in the algorithm, Forward's
Propinqua still works just about as well as forward selection with my specifically assigned datasets. Perhaps this is why:
\begin{itemize}
\item As can be seen in the results (detailed later in this report), Forward
  Selection was more accurate than backward elimination on both datasets. Forward selection performing better indicates that the features in the
  datasets were more independent than correlated.
\item Because the features were more independent than related, \textbf{the risk
    of not exploring a feature subset that produces a pair or set of features that work
    very well together is significantly reduced}.
\item So conversely, on a dataset with highly related features, Forward's
  Propinqua will perform terribly.
\end{itemize}

\subsection{Proof of Reduced Cycle Time}

Figure 1 (pg.4) is a chart of Cycle Times on the large and small datasets between forward
selection and Propinqua.

\section{Results}

My implementation of the assignment's results in comparison to Professor Keogh's
key:
\begin{itemize}
  
\item The Small Dataset, best features:
  \begin{itemize}
  \item \textbf{Forward Selection:} [9, 6, 1], Reported accuracy: 95.960%
  \item \textbf{Backward Elimination:} [1, 6, 8, 9, 10], Reported accuracy: 90.909%
  \item \textbf{Propinqua:} [9, 6, 1], Reported accuracy: 95.960%
  \item \textbf{Professor Keogh's Key:} [9, 3, 6] with 3 being a weak feature
  \end{itemize}
   With respect to Professor Keogh's Key, Forward Selection was better
   than Backward Elimination, yielding two correct features and one spurious
   feature. It missed the weak good feature. Figure 2 (pg.5) was based on
   \textbf{CS170Smalltestdata\_\_32.txt}. 
  
\item The Large Dataset, best features:
  \begin{itemize}
  \item \textbf{Forward Selection:} [24, 19, 25], Reported accuracy: 97.980%
  \item \textbf{Backward Elimination:} [1, 3, 5, 6, 7, 8, 10, 12, 13, 14, 15,
    16, 17, 20, 21, 22, 24, 25, 26, 27, 29, 30, 32, 34, 35, 36, 38, 40, 42, 43,
    44, 45, 46, 47, 48, 49, 50], Reported accuracy: 86.869%
  \item \textbf{Propinqua:} [24, 19, 25], Reported accuracy: 97.980%
  \item \textbf{Professor Keogh's Key:} [24, 34, 19]
    \end{itemize}
\end{itemize}
  With respect to Professor Keogh's Key, Forward Selection was better than
  Backward Elimination, yielding two correct features and one spurious feature.
  It missed one feature. Figures 3-4 (pgs.6-7) was based on
  \textbf{CS170BIGtestdata\_\_71.txt}. 

\section{Conclusion}
Considering the results and performances of forward selection and backward
elimination, it can be said that:
\begin{itemize}
\item Depending on how correlated the features are, either forward selection
  or backward elimination will perform better than the other. Forward
  selection is more accurate when features are highly independent, and
  backward elimination is more accurate when features are highly related.
  Regarding the given datasets \textbf{CS170BIGtestdata\_\_71.txt} and
  \textbf{CS170Smalltestdata\_\_32.txt}, forward selection performed better.
\item Returned feature sets may vary based on the implementation of a given
  algorithm, due to the randomness of each. However, if the search algorithms
  are implemented correctly, some strong features should still be detected.
\item Forward's Propinqua exploits the nature of feature sets with independent
  features. In feature sets with independent features, finding the best feature
  set can be made faster whilst still using the same intuition behind forward
  selection by stopping the search with a subset of features that produces an
  accuracy lower than the current best global accuracy. While Forward's
  Propinqua remains sensitive to local maxima, the liklihood of running into
  local maxima is not as high when used on feature sets with independent features.
 
\end{itemize}

\section{Figures: Charts and Graphs}

\begin{figure}[!h]
  \centering
  \includegraphics[width=8cm,keepaspectratio]{AvgCycleTime.png}
  \caption{Average Cycle Times for Forward Selection and Propinqua. DISCLAIMER: A more accurate estimate of the true cycle time requires many, many more
recordings of elapsed time. However, due to time constraints, the above can be
held as a loose approximation.}
\end{figure}

   \begin{figure}[!h]
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{SmallDataSetGraph.png}
  \caption{Small Dataset and Graph}
\end{figure}

\begin{sidewaysfigure}
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{LargeTestDataGraph.png}
  \caption{Graph based on Large Dataset}
\end{sidewaysfigure}

\begin{sidewaysfigure}
  \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{LargeTestData.png}  
  \caption{Large Dataset Data}
\end{sidewaysfigure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TRACEBACK TIME
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%CODE SHENANIGANS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}